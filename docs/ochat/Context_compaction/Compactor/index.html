<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Compactor (ochat.Context_compaction.Compactor)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Context_compaction</a> &#x00BB; Compactor</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Context_compaction.Compactor</span></code></h1><p>Conversationâ€“history compactor.</p><p><code>Compactor</code> glues together <a href="../Config/index.html"><code>Context_compaction.Config</code></a>, <a href="../Relevance_judge/index.html"><code>Context_compaction.Relevance_judge</code></a>, and <a href="../Summarizer/index.html"><code>Context_compaction.Summarizer</code></a>. Its sole purpose is to trim a potentially long chat transcript down to a size that comfortably fits within the LLMâ€™s context window while keeping the essence of the conversation intact.</p><p>The default pipeline is deliberately lightweight â€“ no tokeniser is needed and the function always finishes in <em>O(n)</em> time where <code>n</code> is the number of messages:</p><ol><li>Load user overrides via <a href="../Config/index.html#val-load"><code>Config.load</code></a>.</li><li>Convert each <a href="../../Openai/Responses/Item/index.html"><code>Openai.Responses.Item</code></a> to plain text and retain only those whose importance score, as judged by <a href="../Relevance_judge/index.html#val-is_relevant"><code>Relevance_judge.is_relevant</code></a>, meets or exceeds <a href="../Config/index.html#type-t.relevance_threshold"><code>Config.t.relevance_threshold</code></a>.</li><li>Pass the retained sub-sequence to <a href="../Summarizer/index.html#val-summarise"><code>Summarizer.summarise</code></a> and truncate the resulting summary to <a href="../Config/index.html#type-t.context_limit"><code>Config.t.context_limit</code></a> characters â€“ the rule-of-thumb <em>1 char â‰ˆ 1 token</em> has proven robust enough in practice.</li><li>Return a new history consisting of the original first item (to keep the system prompt) plus **at most one** additional <code>`system`</code> message containing the summary.</li></ol><p>The entire pipeline is exception-safe: any internal failure causes the function to fall back to the identity transformation and return the original <code>history</code>.</p></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#typical-usage">Typical usage</a></li></ul></nav></div><div class="odoc-content"><h2 id="typical-usage"><a href="#typical-usage" class="anchor"></a>Typical usage</h2><pre class="language-ocaml"><code>  let compacted =
    Context_compaction.Compactor.compact_history
      ~env:(Some stdenv)   (* pass Eio capabilities when available *)
      ~history
  in
  send_to_llm (compacted @ new_user_messages)</code></pre><div class="odoc-spec"><div class="spec value anchored" id="val-compact_history"><a href="#val-compact_history" class="anchor"></a><code><span><span class="keyword">val</span> compact_history : 
  <span><span class="label">env</span>:<span><span class="xref-unresolved">Eio_unix</span>.Stdenv.base option</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">history</span>:<span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span></code></div><div class="spec-doc"><p><code>compact_history ~env ~history</code> returns a condensed replacement for <code>history</code>.</p><p>The result is guaranteed to:</p><ul><li>start with the original first item;</li><li>contain <em>at most</em> one additional <code>`system`</code> message with a summary;</li><li>respect <a href="../Config/index.html#type-t.context_limit"><code>Config.t.context_limit</code></a> (character budget).</li></ul><p>Parameters</p><ul><li><code>env</code> â€“ optional <code>Eio_unix.Stdenv.base</code>. When <code>Some</code>, the pipeline invokes the OpenAI API; when <code>None</code> it falls back to deterministic offline stubs.</li><li><code>history</code> â€“ full conversation transcript to be compacted.</li></ul><p>Never raises â€“ on error the original <code>history</code> is returned verbatim.</p></div></div></div></body></html>
