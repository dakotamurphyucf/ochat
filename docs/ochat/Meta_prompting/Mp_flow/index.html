<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Mp_flow (ochat.Meta_prompting.Mp_flow)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Meta_prompting</a> &#x00BB; Mp_flow</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Meta_prompting.Mp_flow</span></code></h1><p>Recursive meta-prompting orchestration helpers.</p><p>The module exposes two convenience functions that wrap the lower-level <a href="../Recursive_mp/index.html"><code>Recursive_mp</code></a> API with sensible defaults so that callers can refine an initial prompt in only a few lines of code:</p><p>â€¢ <a href="#val-first_flow"><code>first_flow</code></a> â€“ refines a *general* prompt. â€¢ <a href="#val-tool_flow"><code>tool_flow</code></a> â€“ refines a *tool description* prompt that adheres to the OpenAI JSON-tool-calling guidelines.</p><p>Both helpers run several refinement iterations in which they 1. ask a *proposer* LLM to transform the prompt, 2. score the candidate with a *reward-model evaluator*, and 3. keep the best-scoring version.</p><p>The loop terminates once the score plateaus or the maximum number of iterations (currently 5) is reached. All network IO is executed inside the caller-supplied <code>Eio_unix.Stdenv.base</code> environment so that the library can be embedded into any existing Eio event loop.</p><p>Neither function raises; transient errors (e.g. API rate limits) are handled internally by falling back to conservative defaults.</p></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#api">API</a></li><li><a href="#api_2">API</a></li></ul></nav></div><div class="odoc-content"><h2 id="api"><a href="#api" class="anchor"></a>API</h2><h2 id="api_2"><a href="#api_2" class="anchor"></a>API</h2><div class="odoc-spec"><div class="spec value anchored" id="val-first_flow"><a href="#val-first_flow" class="anchor"></a><code><span><span class="keyword">val</span> first_flow : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">task</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">prompt</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?action</span>:<a href="../Context/index.html#type-action">Context.action</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?use_meta_factory_online</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  string</span></code></div><div class="spec-doc"><p><code>first_flow ~env ~task ~prompt ?action ()</code> refines <code>prompt</code> for the user task <code>task</code> and returns the improved prompt text.</p><p>Parameters: â€¢ <code>env</code> â€“ capability record obtained from <code>Eio_main.run</code>; required for talking to external services. â€¢ <code>task</code> â€“ natural-language description of the task the refined prompt should solve. â€¢ <code>prompt</code> â€“ initial draft prompt. â€¢ <code>?action</code> â€“ whether the agent should <code>Generate</code> a new prompt or <code>Update</code> an existing one. Defaults to <a href="../Context/index.html#type-action.Generate"><code>Context.action.Generate</code></a>.</p><p>Return value: the refined prompt **body** (header and footnotes are stripped) ready to be sent to a language model.</p><p>Toggle: â€¢ <code>?use_meta_factory_online</code> â€“ when <code>true</code>, use the online meta-prompt factory iteration strategy to propose edits (LLM-backed). Defaults to <code>true</code>.</p><p>Example:</p><pre class="language-ocaml"><code> let refined =
     Mp_flow.first_flow
       ~env
       ~task:&quot;Summarise the linked article&quot;
       ~prompt:&quot;Read the article and summarise the key points.&quot;
       ()
   in
   print_endline refined </code></pre></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-tool_flow"><a href="#val-tool_flow" class="anchor"></a><code><span><span class="keyword">val</span> tool_flow : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">task</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">prompt</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?action</span>:<a href="../Context/index.html#type-action">Context.action</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?use_meta_factory_online</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  string</span></code></div><div class="spec-doc"><p>Same as <a href="#val-first_flow"><code>first_flow</code></a> but specialised for prompts that describe a JSON-serialised *tool*. Internally the evaluator rubric switches to <a href="../Evaluator/Tool_description_reward_model_judge/index.html"><code>Evaluator.Tool_description_reward_model_judge</code></a> and the context field <code>prompt_type</code> is set to <a href="../Context/index.html#type-prompt_type.Tool"><code>Context.prompt_type.Tool</code></a> so that the proposer and executor templates match OpenAIâ€™s function-calling guidelines.</p><p>Optional parameters mirror <a href="#val-first_flow"><code>first_flow</code></a>: â€¢ <code>?action</code> â€“ choose between prompt generation or in-place update. â€¢ <code>?use_meta_factory_online</code> â€“ enable the online multi-candidate proposer strategy.</p><p>Example:</p><pre class="language-ocaml"><code> let refined_tool =
     Mp_flow.tool_flow
       ~env
       ~task:&quot;Expose a translate function&quot;
       ~prompt:&quot;Translate between English and French.&quot;
       ()
   in
   print_endline refined_tool </code></pre></div></div></div></body></html>
