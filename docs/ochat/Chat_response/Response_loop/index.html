<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Response_loop (ochat.Chat_response.Response_loop)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_response</a> &#x00BB; Response_loop</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_response.Response_loop</span></code></h1><p>Synchronous *response loop* for ChatMarkdown conversations.</p><p><code>Response_loop</code> is the **blocking** counterpart to <code>Fork.run_stream</code>. It keeps forwarding the current conversation <code>history</code> to the OpenAI *chat/completions* endpoint, resolves every tool invocation requested by the model through the user-supplied <code>tool_tbl</code>, and stops only when the modelâ€™s last turn contains **no** <a href="../../Openai/Responses/Item/index.html#type-t.Function_call"><code>Openai.Responses.Item.t.Function_call</code></a> entries.</p><p>The helper is used internally by <a href="../Driver/index.html"><code>Driver</code></a> (CLI &amp; tests) and by nested agents spawned through the <code>fork</code> tool, but it can be called directly when your program does **not** need incremental streaming updates.</p></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#high-level-algorithm">High-level algorithm</a></li></ul></nav></div><div class="odoc-content"><h2 id="high-level-algorithm"><a href="#high-level-algorithm" class="anchor"></a>High-level algorithm</h2><p>1. Push <code>history</code> to the backend via <a href="../../Openai/Responses/index.html#val-post_response"><code>Openai.Responses.post_response</code></a>. 2. Append the resulting <code>output</code> items to <code>history</code>. 3. Collect every <code>`Function_call`</code> item; if the list is empty, return. 4. For each call, look up the OCaml implementation in <code>tool_tbl</code>, execute it, wrap its textual result in a <code>`Function_call_output`</code> placeholder, and append it to <code>history</code>. 5. Repeat from step 1.</p><p>The function is pure except for the side-effects performed by the tools it invokes. Errors raised by those tools or by the underlying HTTP client are propagated to the caller unchanged.</p><div class="odoc-spec"><div class="spec value anchored" id="val-run"><a href="#val-run" class="anchor"></a><code><span><span class="keyword">val</span> run : 
  <span><span class="label">ctx</span>:<span><span>&lt; net : <span><span class="type-var">_</span> <span class="xref-unresolved">Eio</span>.Net.t</span>.. &gt;</span> <a href="../Ctx/index.html#type-t">Ctx.t</a></span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?temperature</span>:float <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?max_output_tokens</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?tools</span>:<span><a href="../../Openai/Responses/Request/Tool/index.html#type-t">Openai.Responses.Request.Tool.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?reasoning</span>:<a href="../../Openai/Responses/Request/Reasoning/index.html#type-t">Openai.Responses.Request.Reasoning.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?fork_depth</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?history_compaction</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">model</span>:<a href="../../Openai/Responses/Request/index.html#type-model">Openai.Responses.Request.model</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">tool_tbl</span>:<span><span>(string, <span>string <span class="arrow">&#45;&gt;</span></span> string)</span> <span class="xref-unresolved">Core</span>.Hashtbl.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span></code></div><div class="spec-doc"><p><code>run ~ctx ?temperature ?max_output_tokens ?tools ?reasoning ?fork_depth ?history_compaction ~model ~tool_tbl history</code></p><p>Expands <code>history</code> until the assistantâ€™s most recent reply contains **no** <a href="../../Openai/Responses/Item/index.html#type-t.Function_call"><code>Openai.Responses.Item.t.Function_call</code></a> items and returns the concatenated list of conversation items.</p><p>Parameters</p><p>â€¢ <code>ctx</code> â€“ immutable execution context providing a network handle (<code>net</code>), a base directory (<code>dir</code>) and a shared cache.</p><p>â€¢ <code>temperature</code> â€“ optional sampling temperature passed verbatim to the model.</p><p>â€¢ <code>max_output_tokens</code> â€“ per-request upper bound on generated tokens.</p><p>â€¢ <code>tools</code> â€“ list of tool definitions forwarded unchanged so the model can invoke them.</p><p>â€¢ <code>reasoning</code> â€“ request that the model emits <code>`Reasoning`</code> blocks.</p><p>â€¢ <code>fork_depth</code> â€“ internal recursion counter used when the loop is entered via the built-in <code>fork</code> tool (default = 0). External callers should leave the default.</p><p>â€¢ <code>history_compaction</code> â€“ when <code>true</code>, redundant <code>read_file</code>/<code>`Function_call_output`</code> pairs are collapsed so only the most recent version of each file is re-sent (see <a href="../Compact_history/index.html#val-collapse_read_file_history"><code>Compact_history.collapse_read_file_history</code></a>).</p><p>â€¢ <code>model</code> â€“ OpenAI model used for **every** iteration.</p><p>â€¢ <code>tool_tbl</code> â€“ mapping *tool-name â†¦ implementation*. The table **must** contain <code>&quot;fork&quot;</code> â†¦ <a href="../Fork/index.html#val-execute"><code>Fork.execute</code></a> so nested agents can run.</p><p>â€¢ <code>history</code> â€“ full conversation so far (user messages, assistant replies, previous tool outputs, â€¦).</p><p>Return value</p><p>Extended conversation that includes every assistant reply and <code>`Function_call_output`</code> produced while the loop was active.</p><ul class="at-tags"><li class="raises"><span class="at-tag">raises</span> <code>Not_found</code> <p>if the model produces a tool name that is not present in <code>tool_tbl</code>.</p></li></ul></div></div></div></body></html>
