<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Response_loop (ochat.Chat_response.Response_loop)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.0.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_response</a> &#x00BB; Response_loop</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_response.Response_loop</span></code></h1></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#example">Example</a></li></ul></nav></div><div class="odoc-content"><div class="odoc-spec"><div class="spec module anchored" id="module-Res"><a href="#module-Res" class="anchor"></a><code><span><span class="keyword">module</span> Res</span><span> = <a href="../../Openai/Responses/index.html">Openai.Responses</a></span></code></div></div><p>Automated completion loop.</p><p><code>Response_loop</code> keeps calling the OpenAI *chat/completions* endpoint until the conversation is *quiescent* â€“ i.e. the assistantâ€™s most recent reply no longer contains any <a href="../../Openai/Responses/Item/index.html#type-t.Function_call"><code>Res.Item.t.Function_call</code></a> requests.</p><p>At each iteration the algorithm:</p><p>1. Sends the current <code>history</code> to the API. 2. Appends all returned items to the conversation. 3. Executes every <code>`Function_call`</code> with the implementation found in <code>tool_tbl</code>, turning the textual result into a synthetic <a href="../../Openai/Responses/Item/index.html#type-t.Function_call_output"><code>Res.Item.t.Function_call_output</code></a> item. 4. Repeats from step 1 until no pending calls remain.</p><p>The helper is **synchronous** and therefore primarily used by non-streaming code paths such as the CLI or unit-tests. A streaming variant with real-time callbacks lives in <code>Fork.run_stream</code>.</p><h2 id="example"><a href="#example" class="anchor"></a>Example</h2><p>Executing a simple loop that has access to a custom <code>grep</code> tool and to <code>Fork.execute</code> for nested assistants:</p><pre class="language-ocaml"><code>{
  let tool_tbl = String.Table.create () in
  Hashtbl.set tool_tbl ~key:&quot;grep&quot; ~data:grep_tool;
  Hashtbl.set tool_tbl ~key:&quot;fork&quot; ~data:Fork.execute;

  let final_history =
    Response_loop.run
      ~ctx
      ~model:Res.Request.Gpt4
      ~tool_tbl
      initial_history
  in
  (* [final_history] now contains assistant replies and tool outputs *)
}</code></pre><p>Response_loop â€“ repeat until no pending function calls</p><p>A generic helper that keeps calling the OpenAI model until the conversation reaches a *quiescent* state â€“ i.e. the last response contains no <code>`Function_call`</code> items. At each iteration the newly requested calls are resolved through the <code>`tool_tbl`</code> mapping and the resulting <code>`Function_call_output`</code> items are appended to the history.</p><p>The algorithm is synchronous and therefore used by non-streaming code paths (CLI, tests). The streaming variant lives in <code>Fork.run_stream</code>.</p><p><code>run ~ctx ?temperature ?max_output_tokens ?tools ?reasoning ~model ~tool_tbl history</code> expands <code>history</code> until the last assistant message contains **no** <code>`Function_call`</code> item.</p><p>Parameters: â€¢ <code>ctx</code> â€“ immutable context that provides network access, current directory and a shared cache. â€¢ <code>?temperature</code> â€“ sampling temperature forwarded verbatim to the model (defaults to the server-side value). â€¢ <code>?max_output_tokens</code> â€“ per-request upper bound on generated tokens. â€¢ <code>?tools</code> â€“ flat list of available tools, forwarded unchanged so the model can call them. â€¢ <code>?reasoning</code> â€“ request whether the model should emit <code>`Reasoning`</code> blocks. â€¢ <code>model</code> â€“ OpenAI model used for **every** iteration. â€¢ <code>tool_tbl</code> â€“ mapping from tool names to implementations. The table **must** hold a <code>&quot;fork&quot;</code> entry pointing at <code>Fork.execute</code> so the built-in <code>fork</code> tool works recursively. â€¢ <code>history</code> â€“ full conversation to date (user messages, assistant replies, previous tool outputs â€¦).</p><p>Returns: the extended conversation made of the original <code>history</code> followed by every newly generated item.</p><p>Complexity: O(kÂ·m) API round-trips where *k* is the maximum nesting depth of function calls and *m* the size of the largest reply.</p><ul class="at-tags"><li class="raises"><span class="at-tag">raises</span> <code>Not_found</code> <p>if a function name produced by the model is **not** present in <code>tool_tbl</code>.</p></li></ul><div class="odoc-spec"><div class="spec value anchored" id="val-run"><a href="#val-run" class="anchor"></a><code><span><span class="keyword">val</span> run : 
  <span><span class="label">ctx</span>:<span><span>&lt; net : <span><span>[&gt; <span><span>[&gt; `Generic ]</span> <span class="xref-unresolved">Eio</span>.Net.ty</span> ]</span> <span class="xref-unresolved">Eio</span>.Net.t</span>.. &gt;</span> <a href="../Ctx/index.html#type-t">Ctx.t</a></span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?temperature</span>:float <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?max_output_tokens</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?tools</span>:<span><a href="../../Openai/Responses/Request/Tool/index.html#type-t">Res.Request.Tool.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?reasoning</span>:<a href="../../Openai/Responses/Request/Reasoning/index.html#type-t">Res.Request.Reasoning.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?fork_depth</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">model</span>:<a href="../../Openai/Responses/Request/index.html#type-model">Res.Request.model</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">tool_tbl</span>:<span><span>(string, <span>string <span class="arrow">&#45;&gt;</span></span> string)</span> <span class="xref-unresolved">Core</span>.Hashtbl.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span><a href="../../Openai/Responses/Item/index.html#type-t">Res.Item.t</a> <span class="xref-unresolved">Base__List</span>.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Item/index.html#type-t">Res.Item.t</a> <span class="xref-unresolved">Base__List</span>.t</span></span></code></div></div></div></body></html>
