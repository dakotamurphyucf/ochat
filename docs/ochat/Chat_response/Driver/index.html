<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Driver (ochat.Chat_response.Driver)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_response</a> &#x00BB; Driver</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_response.Driver</span></code></h1><p>High-level orchestration helpers for ChatMarkdown conversations.</p><p>The <code>Driver</code> module bridges a user-editable <code>.chatmd</code> document and the OpenAI *chat/completions* API. It bundles prompt parsing, caching, tool discovery and the recursive response loop into a few convenience wrappers that can be reused by the CLI, the TUI and nested agents.</p></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#entry-points">Entry points</a></li></ul></nav></div><div class="odoc-content"><h2 id="entry-points"><a href="#entry-points" class="anchor"></a>Entry points</h2><p>â€¢ <a href="#val-run_completion"><code>run_completion</code></a> â€“ blocking, single-turn helper that operates on a document on disk.</p><p>â€¢ <a href="#val-run_completion_stream"><code>run_completion_stream</code></a> â€“ streaming flavour that invokes a callback for every incremental chunk coming from the model so that UIs can render the conversation in real time.</p><p>â€¢ <a href="#val-run_agent"><code>run_agent</code></a> â€“ evaluate a standalone *agent* prompt from within an existing conversation. Used internally by the <code>fork</code> tool but also available to power-users.</p><p>â€¢ <a href="#val-run_completion_stream_in_memory_v1"><code>run_completion_stream_in_memory_v1</code></a> â€“ like <a href="#val-run_completion_stream"><code>run_completion_stream</code></a> but works on an in-memory history instead of a file. Mainly used by the TUI component.</p><div class="odoc-spec"><div class="spec value anchored" id="val-run_agent"><a href="#val-run_agent" class="anchor"></a><code><span><span class="keyword">val</span> run_agent : 
  <span><span class="optlabel">?history_compaction</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">ctx</span>:<span><span class="xref-unresolved">Eio_unix</span>.Stdenv.base <a href="../Ctx/index.html#type-t">Ctx.t</a></span> <span class="arrow">&#45;&gt;</span></span>
  <span>string <span class="arrow">&#45;&gt;</span></span>
  <span><span><a href="../../Prompt/Chat_markdown/index.html#type-content_item">Prompt.Chat_markdown.content_item</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  string</span></code></div><div class="spec-doc"><p><code>run_agent ~ctx prompt_xml items</code> evaluates a *nested agent* inside the current conversation.</p><p><code>run_agent ~ctx prompt_xml items</code> treats <code>prompt_xml</code> as an independent ChatMarkdown document (typically starting with a <code>&amp;lt;system&amp;gt;</code> block and optional configuration) and appends the additional <code>items</code> â€“ usually a user message constructed at runtime â€“ before forwarding everything to the OpenAI endpoint.</p><p>The function blocks until the agent has produced its final assistant answer and returns that answer as plain text (concatenated if multiple messages were emitted).</p><p>It is the callerâ€™s responsibility to ensure that <code>prompt_xml</code> contains any tool declarations required by the agent.</p><p>Complexity: proportional to the number of turns triggered by tool calls inside the agent prompt.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">history_compaction</span> <p>When <code>true</code> the helper collapses repeated reads of the same file so that only the *latest* version is forwarded to the nested agent. This helps keep token usage under control when the agent spawns long chains of tool calls.</p></li></ul></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-run_completion"><a href="#val-run_completion" class="anchor"></a><code><span><span class="keyword">val</span> run_completion : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?prompt_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?parallel_tool_calls</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?meta_refine</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">output_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p><code>run_completion ~env ?prompt_file ?parallel_tool_calls ~output_file ()</code> performs a synchronous, file-based completion.</p><p><code>run_completion ~env ?prompt_file ~output_file ()</code> processes a full ChatMarkdown turn in *blocking* mode. The evolving conversation is read from <code>output_file</code> (created if needed); the final assistant answer is appended back to the same file together with any reasoning blocks and tool-call artefacts. If <code>prompt_file</code> is provided its contents are prepended once at the beginning of the session â€“ handy for templates.</p><p>No value is returned; callers should read <code>output_file</code> afterwards if they need the assistant answer in memory.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">parallel_tool_calls</span> <p>If <code>true</code> (default) tool invocations are executed concurrently (bounded by an internal semaphore). Their outputs are flushed back to the document in the original call order so that the conversation remains deterministic. Set to <code>false</code> for fully sequential execution (useful in tests).</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">meta_refine</span> <p>Enable the *meta-refine* experimental feature that lets the model self-critique and issue follow-up requests. The flag can also be toggled via the <code>OCHAT_META_REFINE</code> environment variable. Defaults to <code>false</code>.</p></li></ul></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-run_completion_stream"><a href="#val-run_completion_stream" class="anchor"></a><code><span><span class="keyword">val</span> run_completion_stream : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?prompt_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?on_event</span>:<span>(<span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a> <span class="arrow">&#45;&gt;</span></span> unit)</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?parallel_tool_calls</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?meta_refine</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?history_compaction</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">output_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p><code>run_completion_stream ~env ?prompt_file ?on_event ?history_compaction ~output_file ()</code> streams assistant deltas and high-level events **as they arrive**.</p><p>Compared to <a href="#val-run_completion"><code>run_completion</code></a> this variant:</p><p>â€¢ Uses the streaming OpenAI API to obtain partial tokens. â€¢ Invokes <code>?on_event</code> for every chunk, letting callers update a TUI or web UI in real time. The default callback ignores events so existing scripts remain unchanged. â€¢ Executes tool calls as soon as they are fully parsed, then continues streaming the response.</p><p>Side-effects mirror <a href="#val-run_completion"><code>run_completion</code></a>: partial messages and reasoning summaries are appended to <code>output_file</code> immediately so the buffer is crash-resistant.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">history_compaction</span> <p>When <code>true</code> the driver collapses redundant file-read entries in the history before forwarding it to the model, saving tokens on long conversations that repeatedly look at the same documents.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">parallel_tool_calls</span> <p>Behaviour identical to the flag of the same name in <a href="#val-run_completion"><code>run_completion</code></a>.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">meta_refine</span> <p>Behaviour identical to the flag of the same name in <a href="#val-run_completion"><code>run_completion</code></a>.</p></li></ul><p>Example â€“ live rendering in the terminal:</p><pre class="language-ocaml"><code>  let on_event = function
    | Responses.Response_stream.Output_text_delta d -&gt;
        Out_channel.output_string stdout d.delta
    | _ -&gt; ()

  Eio_main.run @@ fun env -&gt;
    Driver.run_completion_stream
      ~env
      ~output_file:&quot;conversation.chatmd&quot;
      ~on_event
      ()</code></pre></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-run_completion_stream_in_memory_v1"><a href="#val-run_completion_stream_in_memory_v1" class="anchor"></a><code><span><span class="keyword">val</span> run_completion_stream_in_memory_v1 : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?datadir</span>:<span><span class="xref-unresolved">Eio</span>.Fs.dir_ty <span class="xref-unresolved">Eio</span>.Path.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">history</span>:<span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?on_event</span>:<span>(<span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a> <span class="arrow">&#45;&gt;</span></span> unit)</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?on_fn_out</span>:<span>(<span><a href="../../Openai/Responses/Function_call_output/index.html#type-t">Openai.Responses.Function_call_output.t</a> <span class="arrow">&#45;&gt;</span></span> unit)</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">tools</span>:<span><span><a href="../../Openai/Responses/Request/Tool/index.html#type-t">Openai.Responses.Request.Tool.t</a> list</span> option</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?tool_tbl</span>:<span><span>(string, <span>string <span class="arrow">&#45;&gt;</span></span> string)</span> <span class="xref-unresolved">Core</span>.Hashtbl.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?temperature</span>:float <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?max_output_tokens</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?reasoning</span>:<a href="../../Openai/Responses/Request/Reasoning/index.html#type-t">Openai.Responses.Request.Reasoning.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?history_compaction</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?parallel_tool_calls</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?meta_refine</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?system_event</span>:<span>string <span class="xref-unresolved">Eio</span>.Stream.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?model</span>:<a href="../../Openai/Responses/Request/index.html#type-model">Openai.Responses.Request.model</a> <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span></code></div><div class="spec-doc"><p><code>run_completion_stream_in_memory_v1 ~env ~history ~tools ()</code> streams a ChatMarkdown conversation **held entirely in memory**.</p><p>Compared to <a href="#val-run_completion_stream"><code>run_completion_stream</code></a> this helper:</p><p>â€¢ Accepts an explicit <code>history</code> (list of <a href="../../Openai/Responses/Item/index.html#type-t"><code>Openai.Responses.Item.t</code></a>) instead of reading a `.chatmd` file from disk. â€¢ Returns the *complete* history after all assistant turns and tool calls have been resolved. â€¢ Never touches the filesystem except for the persistent cache under `<code>~/.chatmd</code>`, making it suitable for unit-tests or server back-ends where direct file IO is undesirable.</p><p>Optional callbacks mirror the streaming variant:</p><p>â€¢ <code>?on_event</code> â€“ invoked for each streaming event received from the OpenAI API (token deltas, item completions, â€¦). Defaults to a no-op. â€¢ <code>?on_fn_out</code> â€“ executed after each tool call completes, allowing the caller to react to side-effects without waiting for the final assistant answer.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">env</span> <p>Standard Eio runtime environment.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">history</span> <p>Initial conversation state.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">tools</span> <p>Compile-time list of tool definitions visible to the model. Pass <code>[]</code> for none.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">tool_tbl</span> <p>Optional lookup table generated from <code>tools</code>. The default builds a fresh table via <a href="../../Ochat_function/index.html#val-functions"><code>Ochat_function.functions</code></a> when omitted.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">temperature</span> <p>Temperature override forwarded the OpenAI request.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">max_output_tokens</span> <p>Hard cap on the number of tokens generated by the model per request.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">reasoning</span> <p>Optional reasoning settings forwarded to the API.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">history_compaction</span> <p>If <code>true</code>, the function will compact the history so that multiple calls to the same file are replaced with a single call that points to the latest file content. Outputs for older calls are replaced with a place holder that points to the latest call output (stale) file content removed â€” see newer read_file output later</p></li></ul><ul class="at-tags"><li class="returns"><span class="at-tag">returns</span> <p>The updated <code>history</code>, i.e. the concatenation of the original <code>history</code> and every item produced during the streaming loop.</p></li></ul><ul class="at-tags"><li class="raises"><span class="at-tag">raises</span> <code>Any</code> <p>exception bubbled-up by the OpenAI client or user-supplied tool functions. The function does **not** swallow errors.</p></li></ul></div></div></div></body></html>
