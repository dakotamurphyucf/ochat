<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Fork (ochat.Chat_response.Fork)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_response</a> &#x00BB; Fork</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_response.Fork</span></code></h1><p>Spawn and drive a *forked* assistant session.</p><p>The module implements the runtime side of the built-in <code>file:definitions.ml#module-Fork</code> GPT function called <code>&quot;fork&quot;</code>. Calling <a href="#val-execute"><code>execute</code></a> clones the current conversation, runs a nested completion loop that can itself invoke arbitrary tools â€“ including recursive forks â€“ and finally returns the forked assistantâ€™s textual reply.</p><p>The implementation is split between:</p><p>â€¢ <a href="#val-execute"><code>execute</code></a>, a **blocking** wrapper used by the ordinary non-streaming <a href="../Response_loop/index.html"><code>Response_loop</code></a>; and â€¢ an internal <code>run_stream</code> helper (not exposed in the <code>.mli</code>) that performs the actual streaming request and forwards every event back to the parent so UIs may surface fork progress in real-time.</p></header><div class="odoc-content"><div class="odoc-spec"><div class="spec value anchored" id="val-execute"><a href="#val-execute" class="anchor"></a><code><span><span class="keyword">val</span> execute : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">history</span>:<span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">call_id</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">arguments</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">tools</span>:<span><a href="../../Openai/Responses/Request/Tool/index.html#type-t">Openai.Responses.Request.Tool.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">tool_tbl</span>:<span><span>(string, <span>string <span class="arrow">&#45;&gt;</span></span> string)</span> <span class="xref-unresolved">Base</span>.Hashtbl.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">on_event</span>:<span>(<span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a> <span class="arrow">&#45;&gt;</span></span> unit)</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">on_fn_out</span>:<span>(<span><a href="../../Openai/Responses/Function_call_output/index.html#type-t">Openai.Responses.Function_call_output.t</a> <span class="arrow">&#45;&gt;</span></span> unit)</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?temperature</span>:float <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?max_output_tokens</span>:int <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?reasoning</span>:<a href="../../Openai/Responses/Request/Reasoning/index.html#type-t">Openai.Responses.Request.Reasoning.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  string</span></code></div><div class="spec-doc"><p><code>execute env history call_id arguments tools tool_tbl on_event on_fn_out ()</code> runs a forked agent to completion and returns its final reply.</p><p>Arguments: â€¢ <code>env</code> â€“ the standard Eio environment obtained from <code>Eio_main.run</code>. Only <code>env#net</code> and <code>env#cwd</code> are used internally. â€¢ <code>history</code> â€“ full message history **before** the call to the <code>fork</code> tool. The list is reused verbatim; no deep-copy is needed. â€¢ <code>call_id</code> â€“ identifier of the tool invocation, echoed back in all generated <code>function_call_output</code> items so the parent can match streamed data to the correct call. â€¢ <code>arguments</code> â€“ JSON payload passed to the tool, as received from the OpenAI API. It is decoded with <code>Definitions.Fork.input_of_string</code> to extract the command and positional parameters given to the fork. â€¢ <code>tools</code> â€“ flat list of available tools, forwarded unchanged to the nested completion request so the forked agent has access to the exact same capabilities as its parent. â€¢ <code>tool_tbl</code> â€“ runtime dispatch table mapping tool names to implementations. The table **must** contain an entry called <code>&quot;fork&quot;</code> that points back to <a href="#val-execute"><code>execute</code></a>, otherwise recursive forks will fail with a key-error. â€¢ <code>on_event</code> â€“ callback invoked for **every** low-level streaming event produced by the nested request, including tool-call arguments, deltas and reasoning sections. â€¢ <code>on_fn_out</code> â€“ callback invoked whenever the fork produces a new <code>function_call_output</code> block. This is necessary because assistants frequently stream partial answers before the final message ends.</p><p>Optional parameters: â€¢ <code>?temperature</code> â€“ sampling temperature forwarded to the nested request; defaults to the OpenAI model default. â€¢ <code>?max_output_tokens</code> â€“ explicit upper bound on generated tokens. â€¢ <code>?reasoning</code> â€“ whether reasoning items should be generated (see <a href="../../Openai/Responses/Request/Reasoning/index.html"><code>Openai.Responses.Request.Reasoning</code></a>).</p><p>Returns: the concatenation of all assistant messages generated by the fork *after* the initial history, separated by newlines.</p><p>The call is synchronous: the function only returns once the forked agent has either terminated normally or the OpenAI request hit its server-side token limit.</p><p>Example â€“ spawning a grep helper:</p><pre class="language-ocaml"><code>  let reply =
    Fork.execute
      ~env
      ~history:items
      ~call_id:&quot;42&quot;
      ~arguments:{|
        { &quot;command&quot;: &quot;grep&quot;, &quot;arguments&quot;: [&quot;-n&quot;, &quot;let&quot;, &quot;*.ml&quot;] }
      |}
      ~tools
      ~tool_tbl
      ~on_event:(fun _ -&gt; ())
      ~on_fn_out:(fun _ -&gt; ())
      ()
  in
  print_endline reply</code></pre></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-history"><a href="#val-history" class="anchor"></a><code><span><span class="keyword">val</span> history : 
  <span><span class="label">history</span>:<span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">arguments</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span>string <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span></code></div><div class="spec-doc"><p><code>history ~history ~arguments call_id</code> injects the synthetic *instruction* <code>function_call_output</code> item used by <a href="#val-execute"><code>execute</code></a> at the end of <code>history</code> and returns the resulting list.</p><p>This helper is primarily intended for unit-tests that need to verify the exact message sequence fed to the OpenAI client without running a full completion.</p></div></div></div></body></html>
