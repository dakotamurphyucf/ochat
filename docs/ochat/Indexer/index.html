<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Indexer (ochat.Indexer)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.0.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../';
let search_urls = ['../db.js','../../sherlodoc.js'];
</script><script src="../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../index.html">Index</a> &#x00BB; <a href="../index.html">ochat</a> &#x00BB; Indexer</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Indexer</span></code></h1><p>Hybrid semantic + lexical index over OCaml sources.</p><p>The module exposes a single entry-point <a href="#val-index"><code>index</code></a> that crawls a directory tree, extracts ocamldoc comments from each <code>*.ml</code> and <code>*.mli</code> file, obtains OpenAI embeddings for every documentation snippet, and persists both the dense vectors and a BM-25 paragraph index to disk. The resulting artefacts can later be loaded by <a href="../Vector_db/index.html"><code>Vector_db</code></a> and <a href="../Bm25/index.html"><code>Bm25</code></a> for fast similarity search inside Ochat-style assistants or code-navigation tools.</p><p>Internally the work is split into a producer/consumer pipeline:</p><p>â€¢ A pool of background fibres created via <code>Task_pool</code> traverses the parse tree (see <a href="../Ocaml_parser/index.html"><code>Ocaml_parser</code></a>) and breaks the documentation stream into ~100â€“300-token chunks. Each chunk is tagged with precise location metadata (file, line range, interface vs implementation).</p><p>â€¢ Batches of chunks are sent to the OpenAI *Embeddings* endpoint via <a href="../Openai/Embeddings/index.html"><code>Openai.Embeddings</code></a>. The returned vectors are normalised and written out using <a href="../Vector_db/Vec/index.html#val-write_vectors_to_disk"><code>Vector_db.Vec.write_vectors_to_disk</code></a>.</p><p>â€¢ In parallel the plain text of every snippet is fed into <code>Bm25.create</code> so that keyword-based ranking is also available at query time.</p><p>Concurrency is managed with <code>Eio</code> fibres and <code>Eio.Domain_manager</code>. The function is fully blocking until the index has been flushed to disk.</p></header><div class="odoc-content"><div class="odoc-spec"><div class="spec value anchored" id="val-index"><a href="#val-index" class="anchor"></a><code><span><span class="keyword">val</span> index : 
  <span><span class="label">sw</span>:<span class="xref-unresolved">Eio</span>.Switch.t <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">dir</span>:<span><span class="xref-unresolved">Eio</span>.Fs.dir_ty <span class="xref-unresolved">Eio</span>.Path.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">dm</span>:<span><span class="xref-unresolved">Eio</span>.Domain_manager.ty <span class="xref-unresolved">Eio</span>.Resource.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">net</span>:<span><span class="type-var">_</span> <span class="xref-unresolved">Eio</span>.Net.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">vector_db_folder</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">folder_to_index</span>:string <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p>Blocks until the index has been written. May raise if the OpenAI request fails or the file system is read-only.</p></div></div></div></body></html>
