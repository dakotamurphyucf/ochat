<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>App (ochat.Chat_tui.App)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_tui</a> &#x00BB; App</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_tui.App</span></code></h1><p>Terminal chat application â€“ event-loop, streaming, export, and persistence.</p><p><a href="#"><code>Chat_tui.App</code></a> is the orchestration layer that powers the Ochat terminal UI.</p><p>It wires together:</p><ul><li><a href="../Model/index.html"><code>Chat_tui.Model</code></a> for mutable UI state</li><li><a href="../Controller/index.html"><code>Chat_tui.Controller</code></a> for key handling</li><li><a href="../Renderer/index.html"><code>Chat_tui.Renderer</code></a> for full-screen rendering</li><li><a href="../../Notty_eio/Term/index.html"><code>Notty_eio.Term</code></a> for terminal IO</li><li><a href="../../Chat_response/Driver/index.html"><code>Chat_response.Driver</code></a> for OpenAI streaming and tool execution</li><li><a href="../../Context_compaction/Compactor/index.html"><code>Context_compaction.Compactor</code></a> for user-triggered history compaction</li></ul><p>Use <a href="#val-run_chat"><code>run_chat</code></a> to boot the UI and block until the user quits.</p><p>Most callers should treat everything other than <a href="#val-run_chat"><code>run_chat</code></a> as test-support: these helpers are exposed to enable white-box unit and integration tests of the event-loop and streaming behaviour.</p></header><div class="odoc-content"><div class="odoc-spec"><div class="spec type anchored" id="type-prompt_context"><a href="#type-prompt_context" class="anchor"></a><code><span><span class="keyword">type</span> prompt_context</span><span> = </span><span>{</span></code><ol><li id="type-prompt_context.cfg" class="def record field anchored"><a href="#type-prompt_context.cfg" class="anchor"></a><code><span>cfg : <a href="../../Chat_response/Config/index.html#type-t">Chat_response.Config.t</a>;</span></code><div class="def-doc"><span class="comment-delim">(*</span><p>Behavioural settings (temperature, â€¦)</p><span class="comment-delim">*)</span></div></li><li id="type-prompt_context.tools" class="def record field anchored"><a href="#type-prompt_context.tools" class="anchor"></a><code><span>tools : <span><a href="../../Openai/Responses/Request/Tool/index.html#type-t">Openai.Responses.Request.Tool.t</a> list</span>;</span></code><div class="def-doc"><span class="comment-delim">(*</span><p>Tools exposed to the assistant at runtime.</p><span class="comment-delim">*)</span></div></li><li id="type-prompt_context.tool_tbl" class="def record field anchored"><a href="#type-prompt_context.tool_tbl" class="anchor"></a><code><span>tool_tbl : <span><span>(string, <span>string <span class="arrow">&#45;&gt;</span></span> <a href="../../Openai/Responses/Tool_output/Output/index.html#type-t">Openai.Responses.Tool_output.Output.t</a>)</span>
             <span class="xref-unresolved">Base</span>.Hashtbl.t</span>;</span></code><div class="def-doc"><span class="comment-delim">(*</span><p>Mapping <code>tool_name -&gt; implementation</code>.</p><span class="comment-delim">*)</span></div></li></ol><code><span>}</span></code></div><div class="spec-doc"><p>Runtime artefacts derived from the static chat prompt.</p></div></div><div class="odoc-spec"><div class="spec type anchored" id="type-persist_mode"><a href="#type-persist_mode" class="anchor"></a><code><span><span class="keyword">type</span> persist_mode</span><span> = </span><span>[ </span></code><ol><li id="type-persist_mode.Always" class="def variant constructor anchored"><a href="#type-persist_mode.Always" class="anchor"></a><code><span>| </span><span>`Always</span></code></li><li id="type-persist_mode.Never" class="def variant constructor anchored"><a href="#type-persist_mode.Never" class="anchor"></a><code><span>| </span><span>`Never</span></code></li><li id="type-persist_mode.Ask" class="def variant constructor anchored"><a href="#type-persist_mode.Ask" class="anchor"></a><code><span>| </span><span>`Ask</span></code></li></ol><code><span> ]</span></code></div><div class="spec-doc"><p>Persistence policy to use when the UI terminates.</p><p>The value controls whether a <a href="../../Session/index.html#type-t"><code>Session.t</code></a> snapshot derived from the final <a href="../Model/index.html#type-t"><code>Model.t</code></a> is written back to disk at the end of <a href="#val-run_chat"><code>run_chat</code></a>. The policy is ignored when no <code>session</code> was supplied.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-add_placeholder_thinking_message"><a href="#val-add_placeholder_thinking_message" class="anchor"></a><code><span><span class="keyword">val</span> add_placeholder_thinking_message : <span><a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> unit</span></code></div><div class="spec-doc"><p><code>add_placeholder_thinking_message m</code> appends a transient &quot;(thinkingâ€¦)&quot; assistant message to <code>m</code>. It is removed as soon as the first streaming token arrives.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-add_placeholder_stream_error"><a href="#val-add_placeholder_stream_error" class="anchor"></a><code><span><span class="keyword">val</span> add_placeholder_stream_error : <span><a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> <span>string <span class="arrow">&#45;&gt;</span></span> unit</span></code></div><div class="spec-doc"><p><code>add_placeholder_stream_error model msg</code> appends a transient error message to <code>model</code> so failures during streaming surface in the transcript instead of being silently logged.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-add_placeholder_compact_message"><a href="#val-add_placeholder_compact_message" class="anchor"></a><code><span><span class="keyword">val</span> add_placeholder_compact_message : <span><a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> unit</span></code></div><div class="spec-doc"><p><code>add_placeholder_compact_message model</code> appends a transient &quot;(compactingâ€¦)&quot; assistant message to <code>model</code> while context compaction is running.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-persist_snapshot"><a href="#val-persist_snapshot" class="anchor"></a><code><span><span class="keyword">val</span> persist_snapshot : 
  <span><span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span><a href="../../Session/index.html#type-t">Session.t</a> option</span> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p><code>persist_snapshot env session model</code> persists the in-memory <code>model</code> back into <code>session</code> and writes it to disk.</p><p>The helper copies the canonical history, task list and key/value store from <code>model</code> into the supplied <a href="../../Session/index.html#type-t"><code>Session.t</code></a> and then delegates to <a href="../../Session_store/index.html#val-save"><code>Session_store.save</code></a>. It is a no-op when <code>session</code> is <code>None</code>.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-apply_local_submit_effects"><a href="#val-apply_local_submit_effects" class="anchor"></a><code><span><span class="keyword">val</span> apply_local_submit_effects : 
  <span><span class="label">dir</span>:<span><span class="xref-unresolved">Eio</span>.Fs.dir_ty <span class="xref-unresolved">Eio</span>.Path.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">cache</span>:<a href="../../Chat_response/Cache/index.html#type-t">Chat_response.Cache.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">ev_stream</span>:
    <span><span>(<span>[&gt; `Redraw
     <span><span>| `Stream</span> of <a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a></span>
     <span><span>| `Stream_batch</span> of <span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a> list</span></span>
     <span><span>| `Replace_history</span> of <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span>
     <span><span>| `Function_output</span> of <a href="../../Openai/Responses/Function_call_output/index.html#type-t">Openai.Responses.Function_call_output.t</a></span>
     <span><span>| `Tool_output</span> of <a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a></span> ]</span> <span class="keyword">as</span> 'ev)</span>
      <span class="xref-unresolved">Eio</span>.Stream.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">term</span>:<a href="../../Notty_eio/Term/index.html#type-t">Notty_eio.Term.t</a> <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p>Immediate (synchronous) UI updates that happen right after the user submits the draft but <b>before</b> the OpenAI request is sent.</p><p>The helper:</p><ul><li>snapshots the current draft buffer, converts it into a user history item (plain text or raw-XML, depending on <code>Model.draft_mode</code>);</li><li>appends a renderable user message to <a href="../Model/index.html#type-t.messages"><code>Model.t.messages</code></a> and updates <a href="../Model/index.html#type-t.history_items"><code>Model.t.history_items</code></a>;</li><li>clears <a href="../Model/index.html#type-t.input_line"><code>Model.t.input_line</code></a>, resets the caret and enables <a href="../Model/index.html#type-t.auto_follow"><code>Model.t.auto_follow</code></a>;</li><li>scrolls the viewport so the freshly submitted message is visible;</li><li>injects a transient &quot;(thinkingâ€¦)&quot; assistant placeholder; and</li><li>pushes a <code>`Redraw</code> request on <code>ev_stream</code>.</li></ul><p>Actual network IO and streaming are delegated to <a href="#val-handle_submit"><code>handle_submit</code></a>.</p><p>This function performs no network IO.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-handle_submit"><a href="#val-handle_submit" class="anchor"></a><code><span><span class="keyword">val</span> handle_submit : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">ev_stream</span>:
    <span><span>(<span>[&gt; `Redraw
     <span><span>| `Stream</span> of <a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a></span>
     <span><span>| `Stream_batch</span> of <span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Openai.Responses.Response_stream.t</a> list</span></span>
     <span><span>| `Replace_history</span> of <span><a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a> list</span></span>
     <span><span>| `Function_output</span> of <a href="../../Openai/Responses/Function_call_output/index.html#type-t">Openai.Responses.Function_call_output.t</a></span>
     <span><span>| `Tool_output</span> of <a href="../../Openai/Responses/Item/index.html#type-t">Openai.Responses.Item.t</a></span> ]</span> <span class="keyword">as</span> 'ev)</span>
      <span class="xref-unresolved">Eio</span>.Stream.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">system_event</span>:<span>string <span class="xref-unresolved">Eio</span>.Stream.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">prompt_ctx</span>:<a href="#type-prompt_context">prompt_context</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">datadir</span>:<span><span class="xref-unresolved">Eio</span>.Fs.dir_ty <span class="xref-unresolved">Eio</span>.Path.t</span> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">parallel_tool_calls</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">history_compaction</span>:bool <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p>Launch an <b>asynchronous</b> OpenAI completion request and stream the results back into the UI.</p><p>A fresh <code>Eio.Switch.t</code> is created so the request can be cancelled independently via [Esc]. All network IO happens in spawned fibres â€“ the call therefore returns <i>immediately</i> and never blocks the event loop.</p><p>Batching strategy</p><p>Individual token events arrive at sub-millisecond latency which is far too fast for human eyes. To avoid wasting CPU cycles (and battery!) contiguous <code>`Stream</code> events are coalesced into a single <code>`Stream_batch</code> if they arrive within a small time-window (12 ms by default). The window can be tweakedâ€”primarily for benchmarkingâ€”via the environment variable <code>$OCHAT_STREAM_BATCH_MS</code> (valid range 1â€“50 ms).</p><p>Tool output items are *not* batched because they are rare and often trigger further side-effects.</p><p>Parameters</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">env</span> <p>The current <code>Eio.Stdenv.t</code> (typically <code>Eio_unix.Stdenv.base</code>).</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">model</span> <p>Mutable state record that will be updated while the stream progresses.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">ev_stream</span> <p>Event queue shared with the main UI loop. Both <code>`Stream</code> and <code>`Stream_batch</code> events are emitted here, as well as <code>`Tool_output</code> values.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">system_event</span> <p>Out-of-band messages (e.g. notes from the user) that should appear in the assistantâ€™s context but must not be rendered in the viewport.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">prompt_ctx</span> <p>Runtime artefacts (model temperature, tool declarations, â€¦) derived from the static prompt.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">datadir</span> <p>Directory used to store temporary artefacts such as the response cache and tool outputs.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">parallel_tool_calls</span> <p>When <code>true</code> (default) tool invocations are evaluated concurrently; otherwise they run sequentially.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">history_compaction</span> <p>Forwarded to <a href="../../Chat_response/Driver/index.html#val-run_completion_stream_in_memory_v1"><code>Chat_response.Driver.run_completion_stream_in_memory_v1</code></a> to enable its lightweight history-compaction pipeline (collapsing redundant file-read entries).</p></li></ul></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-run_chat"><a href="#val-run_chat" class="anchor"></a><code><span><span class="keyword">val</span> run_chat : 
  <span><span class="label">env</span>:<span class="xref-unresolved">Eio_unix</span>.Stdenv.base <span class="arrow">&#45;&gt;</span></span>
  <span><span class="label">prompt_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?session</span>:<a href="../../Session/index.html#type-t">Session.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?export_file</span>:string <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?persist_mode</span>:<a href="#type-persist_mode">persist_mode</a> <span class="arrow">&#45;&gt;</span></span>
  <span><span class="optlabel">?parallel_tool_calls</span>:bool <span class="arrow">&#45;&gt;</span></span>
  <span>unit <span class="arrow">&#45;&gt;</span></span>
  unit</span></code></div><div class="spec-doc"><p>Boot the TUI and block until the user terminates the program.</p><p>Calling <code>run_chat ~env ~prompt_file ()</code> is the primary way to start an interactive Ochat session from an executable. The function initialises a full-screen <a href="../../Notty_eio/Term/index.html"><code>Notty_eio.Term</code></a>, parses the ChatMarkdown prompt, builds an initial <a href="../Model/index.html#type-t"><code>Model.t</code></a> and then runs the main event-loop until the user quits.</p><p>On shutdown the helper can:</p><ul><li>optionally export the full conversation as ChatMarkdown (either automatically or after a <code>y/N</code> prompt, depending on how the user exited the UI and the value of <code>?export_file</code>);</li><li>optionally persist the session snapshot according to <code>?persist_mode</code>.</li></ul><p>Parameters:</p><ul><li><code>env</code> â€“ The standard environment supplied by <code>Eio_main.run</code>.</li><li><code>prompt_file</code> â€“ Path to the <code>*.chatmd*</code> prompt that seeds the conversation, declares tools and configures default model settings.</li><li><code>session</code> â€“ Optional persisted session that should be resumed. When present, its history, tasks and key/value store take precedence over the defaults from <code>prompt_file</code>.</li><li><code>export_file</code> â€“ Optional override for the ChatMarkdown export path. When omitted the prompt file path is reused.</li><li><code>persist_mode</code> â€“ Policy controlling whether the session snapshot is written back on exit. Defaults to <code>`Ask</code>.</li><li><code>parallel_tool_calls</code> â€“ Allow multiple tool calls to run in parallel (default: <code>true</code>).</li></ul></div></div></div></body></html>
