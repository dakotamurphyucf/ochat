<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Stream (ochat.Chat_tui.Stream)</title><meta charset="utf-8"/><link rel="stylesheet" href="../../../odoc.support/odoc.css"/><meta name="generator" content="odoc 3.1.0"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><script src="../../../odoc.support/highlight.pack.js"></script><script>hljs.initHighlightingOnLoad();</script><script>let base_url = '../../../';
let search_urls = ['../../db.js','../../../sherlodoc.js'];
</script><script src="../../../odoc.support/odoc_search.js" defer="defer"></script></head><body class="odoc"><nav class="odoc-nav"><a href="../index.html">Up</a> â€“ <a href="../../../index.html">Index</a> &#x00BB; <a href="../../index.html">ochat</a> &#x00BB; <a href="../index.html">Chat_tui</a> &#x00BB; Stream</nav><div class="odoc-search"><div class="search-inner"><input class="search-bar" placeholder="ðŸ”Ž Type '/' to search..."/><div class="search-snake"></div><div class="search-result"></div></div></div><header class="odoc-preamble"><h1>Module <code><span>Chat_tui.Stream</span></code></h1><p>Translate raw OpenAI streaming events into declarative patch commands.</p><p>The <a href="#"><code>Chat_tui.Stream</code></a> module converts the incremental events emitted by the ChatCompletions *stream* endpoint of the OpenAI API â€“ represented by <a href="../../Openai/Responses/Response_stream/index.html#type-t"><code>Openai.Responses.Response_stream.t</code></a> â€“ into the declarative <a href="../Types/index.html#type-patch"><code>Types.patch</code></a> language that the rest of the terminal UI understands.</p></header><div class="odoc-tocs"><nav class="odoc-toc odoc-local-toc"><ul><li><a href="#design-goals">Design goals</a></li><li><a href="#pipeline-change-2024-07">Pipeline change 2024-07</a></li></ul></nav></div><div class="odoc-content"><h2 id="design-goals"><a href="#design-goals" class="anchor"></a>Design goals</h2><ul><li>*Side-effect minimisation* â€“ the primary result of every helper is a list of <a href="../Types/index.html#type-patch"><code>Types.patch</code></a> values. A small amount of internal book-keeping on the supplied <a href="../Model/index.html#type-t"><code>Model.t</code></a> (e.g. toggling <a href="../Model/index.html#type-t.active_fork"><code>Model.t.active_fork</code></a>) is still required. These mutations are confined to metadata fields and never alter the immutable message history that is expressed via patches.</li></ul><ul><li>*Single responsibility* â€“ this module is the _only_ place that knows how to interpret the many concrete variants of <a href="../../Openai/Responses/Response_stream/index.html#type-t"><code>Openai.Responses.Response_stream.t</code></a>. The remainder of the code base deals solely with patches and therefore remains stable when OpenAI adds new streaming event kinds.</li></ul><ul><li>*Forward compatibility* â€“ once the planned migration to an immutable model representation lands, even the remaining book-keeping updates will move into patches so that the module regains full referential transparency.</li></ul><h2 id="pipeline-change-2024-07"><a href="#pipeline-change-2024-07" class="anchor"></a>Pipeline change 2024-07</h2><p>â€¢ **Raw text deltas** â€“ the stream handler now emits *unsanitised* raw text via <a href="../Types/index.html#type-patch.Append_text"><code>Types.patch.Append_text</code></a> patches. No UTF-8 validation or word-wrapping happens at this stage.</p><p>â€¢ **Batching &amp; coalescing** â€“ <a href="../App/index.html"><code>Chat_tui.App</code></a> groups the deltas in its <code>Stream_batch</code> handler and merges adjacent <code>Append_text</code> patches that target the same buffer. This keeps the patch volume small without compromising incremental updates.</p><p>â€¢ **Renderer-side sanitisation** â€“ invalid byte sequences are removed exactly once during rendering, together with word-wrapping. The change reduces cache invalidations and re-wraps per frame while leaving the visual output unchanged.</p><div class="odoc-spec"><div class="spec module anchored" id="module-Res"><a href="#module-Res" class="anchor"></a><code><span><span class="keyword">module</span> Res</span><span> = <a href="../../Openai/Responses/index.html">Openai.Responses</a></span></code></div></div><div class="odoc-spec"><div class="spec module anchored" id="module-Res_stream"><a href="#module-Res_stream" class="anchor"></a><code><span><span class="keyword">module</span> Res_stream</span><span> = <a href="../../Openai/Responses/Response_stream/index.html">Openai.Responses.Response_stream</a></span></code></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-handle_fn_out"><a href="#val-handle_fn_out" class="anchor"></a><code><span><span class="keyword">val</span> handle_fn_out : 
  <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../../Openai/Responses/Function_call_output/index.html#type-t">Res.Function_call_output.t</a> <span class="arrow">&#45;&gt;</span></span>
  <span><a href="../Types/index.html#type-patch">Types.patch</a> list</span></span></code></div><div class="spec-doc"><p><code>handle_fn_out ~model out</code> converts a completed tool-call into a patch stream.</p><p>When the assistant finishes executing a function call and returns its textual result, the OpenAI API emits a <a href="../../Openai/Responses/Function_call_output/index.html#type-t"><code>Openai.Responses.Function_call_output.t</code></a>. The helper translates that record into the following patches:</p><p>â€¢ <a href="../Types/index.html#type-patch.Set_function_output"><code>Types.patch.Set_function_output</code></a> â€“ stores <code>out.output</code> under the final message id so that the renderer can display the tool response.</p><p>Additionally, if the call belonged to a *fork* (the assistantâ€™s internal concurrency primitive) the helper clears <a href="../Model/index.html#type-t.active_fork"><code>Model.t.active_fork</code></a>/<a href="../Model/index.html#type-t.fork_start_index"><code>Model.t.fork_start_index</code></a> so that new messages are rendered using the default appearance again.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">model</span> <p>State snapshot that may receive fork-book-keeping updates.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">out</span> <p>Completed function-call record received from the OpenAI API.</p></li></ul><p>@side_effect Mutates <code>model.active_fork</code> and <code>model.fork_start_index</code> to clear the special *fork* colour-coding once the call finishes.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-handle_tool_out"><a href="#val-handle_tool_out" class="anchor"></a><code><span><span class="keyword">val</span> handle_tool_out : <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> <span><a href="../../Openai/Responses/Item/index.html#type-t">Res.Item.t</a> <span class="arrow">&#45;&gt;</span></span> <span><a href="../Types/index.html#type-patch">Types.patch</a> list</span></span></code></div><div class="spec-doc"><p><code>handle_tool_out ~model item</code> is like <a href="#val-handle_fn_out"><code>handle_fn_out</code></a> but accepts the full history item.</p><p>The function currently handles:</p><ul><li><a href="../../Openai/Responses/Item/index.html#type-t.Function_call_output"><code>Openai.Responses.Item.t.Function_call_output</code></a></li><li><a href="../../Openai/Responses/Item/index.html#type-t.Custom_tool_call_output"><code>Openai.Responses.Item.t.Custom_tool_call_output</code></a></li></ul><p>All other items yield <code>[]</code>.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-handle_event"><a href="#val-handle_event" class="anchor"></a><code><span><span class="keyword">val</span> handle_event : <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> <span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Res_stream.t</a> <span class="arrow">&#45;&gt;</span></span> <span><a href="../Types/index.html#type-patch">Types.patch</a> list</span></span></code></div><div class="spec-doc"><p><code>handle_event ~model ev</code> converts a single incremental streaming event into a list of patches.</p><p>The implementation understands (and therefore potentially produces patches for) the following event classes:</p><p>â€¢ <code>Output_text_delta</code> â€“ append assistant text chunks â€¢ <code>Output_item_added</code> â€“ announce new items (messages, reasoning blocks or function calls) and initialise their buffers / metadata â€¢ <code>Output_message</code> â€“ full assistant message delivered in a single event (handled as a sub-variant of <code>Output_item_added</code>) â€¢ <code>Reasoning_summary_text_delta</code> â€“ update tool reasoning sections â€¢ <code>Function_call_arguments_delta</code> / <code>Function_call_arguments_done</code> â€“ stream the argument list of a tool invocation</p><p>For <code>read_file</code> and <code>read_directory</code> calls, the implementation extracts the requested path from the final argument JSON. When tool calls run in parallel, the OpenAI stream may deliver <code>Function_call_output</code> before <code>Function_call_arguments_done</code>; in that case, the stream handler updates the already-rendered tool output metadata and invalidates the per-message render cache so syntax-highlighting can be applied immediately (without waiting for a full history rebuild at turn end).</p><p>All other variants are ignored for now and yield <code>[]</code>.</p><p>The returned list can be empty, contain a single patch, or multiple patches when a more complex update â€“ e.g. buffer initialisation *and* delta append â€“ is required.</p><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">model</span> <p>Mutable UI state used for ancillary book-keeping (forks and reasoning indices).</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">ev</span> <p>Single streaming event decoded from JSON.</p></li></ul><p>@side_effect May update <a href="../Model/index.html#type-t.active_fork"><code>Model.t.active_fork</code></a> and <a href="../Model/index.html#type-t.fork_start_index"><code>Model.t.fork_start_index</code></a>. All other state changes are delivered as patches.</p></div></div><div class="odoc-spec"><div class="spec value anchored" id="val-handle_events"><a href="#val-handle_events" class="anchor"></a><code><span><span class="keyword">val</span> handle_events : <span><span class="label">model</span>:<a href="../Model/index.html#type-t">Model.t</a> <span class="arrow">&#45;&gt;</span></span> <span><span><a href="../../Openai/Responses/Response_stream/index.html#type-t">Res_stream.t</a> list</span> <span class="arrow">&#45;&gt;</span></span> <span><a href="../Types/index.html#type-patch">Types.patch</a> list</span></span></code></div><div class="spec-doc"><p><code>handle_events ~model evs</code> folds <a href="#val-handle_event"><code>handle_event</code></a> over <code>evs</code> and concatenates the resulting patch lists. It exists purely for convenience when a client already has a list of streaming events. The function behaves like:</p><pre class="language-ocaml"><code>  List.concat_map evs ~f:(handle_event ~model)</code></pre><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">model</span> <p>UI state passed through to <a href="#val-handle_event"><code>handle_event</code></a>.</p></li></ul><ul class="at-tags"><li class="parameter"><span class="at-tag">parameter</span> <span class="value">evs</span> <p>List of streaming events to translate.</p></li></ul><p>It does not introduce additional side-effects beyond those already performed by <a href="#val-handle_event"><code>handle_event</code></a>.</p></div></div></div></body></html>
