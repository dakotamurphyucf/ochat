# `Markdown_snippet`

Break Markdown documents into *token-bounded* slices that can be sent to a
language-model embedding API.

The implementation is almost a copy of `Odoc_snippet` with changes for the
less regular structure of hand-written docs.

---

## Chunking rules

* Target window: **64 – 320 tokens** (inclusive).
* Overlap: **64 tokens** (≈20 %).
* Split boundaries:
  * ATX headings (`#`, `##`, …​).
  * Thematic breaks (`---`, `***`, `___`).
  * Blank lines.
  * Fenced code blocks (`` ``` ``) – kept intact.
  * GFM tables (rows starting with `|`).

The logic lives in the internal `Chunker` module and can be reused by callers.

---

## Metadata

```ocaml
type meta = {
  id         : string;        (* md5(meta+body)          *)
  index      : string;        (* index name supplied by user *)
  doc_path   : string;        (* relative path *)
  title      : string option; (* first H1/H2 heading *)
  line_start : int;
  line_end   : int;
}
```

`id` serves as the primary key for persistence and deduplication.

---

## Public function

```ocaml
val slice :
  index_name:string ->
  doc_path:string ->
  markdown:string ->
  tiki_token_bpe:string ->    (* contents of Tikitoken BPE file *)
  unit ->
  (meta * string) list        (* header prepended to body *)
```

The resulting list can be fed directly into `Embed_service`.

---

## Performance notes

* Token counting uses an **LRU cache** (~5000 entries) protected by an
  `Eio.Mutex` – safe to call from multiple fibres.
* When `tikitoken` fails (e.g. malformed UTF-8) a heuristic word-count fallback
  keeps the pipeline running.

---

## Usage example

Below is a minimal end-to-end example that chunks a local `README.md` and
prints the meta-data alongside the first 120 characters of each slice:

```ocaml
open Core

let () =
  let bpe = In_channel.read_all "gpt4.tiktoken" in
  let markdown = In_channel.read_all "README.md" in
  let slices = Markdown_snippet.slice
    ~index_name:"my-project"
    ~doc_path:"README.md"
    ~markdown
    ~tiki_token_bpe:bpe
    ()
  in
  List.iter slices ~f:(fun (meta, text) ->
    printf "%s\n%.120s…\n\n" (Sexp.to_string_hum (Markdown_snippet.Meta.sexp_of_t meta)) text)
```

---

## Known limitations

* **Single-file focus** – `slice` processes one Markdown document at a time.
  Callers are expected to manage directory traversal (see
  `Markdown_crawler`).
* **Approximate token counting for long paragraphs** – For >2 kB strings the
  implementation falls back to a cheap size-based heuristic instead of fully
  invoking Tiktoken.  This trades a small amount of accuracy for significant
  speed-ups.
* **No inline maths or HTML awareness** – The chunker purposefully ignores
  inline constructs.  Very large HTML blocks within Markdown are treated as
  raw paragraphs and may therefore be split mid-tag.

---

## See also

* [`Odoc_snippet`](odoc_snippet.doc.md) – identical logic specialised for OCaml
  API reference generated by `odoc`.
* [`Markdown_indexer`](markdown_indexer.doc.md) – consumes slices and pushes
  them into a vector database backed by OpenAI embeddings.

