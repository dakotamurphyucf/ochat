(** Spawn and drive a *forked* assistant session.

    The module implements the runtime side of the built-in {!file:definitions.ml#module-Fork} GPT
    function called ["fork"].  Calling {!execute} clones the current
    conversation, runs a nested completion loop that can itself invoke
    arbitrary tools – including recursive forks – and finally returns the
    forked assistant’s textual reply.

    The implementation is split between:

    • {!execute}, a **blocking** wrapper used by the ordinary
      non-streaming {!Response_loop}; and
    • an internal [run_stream] helper (not exposed in the [.mli]) that
      performs the actual streaming request and forwards every event back
      to the parent so UIs may surface fork progress in real-time. *)

(** [execute env history call_id arguments tools tool_tbl on_event on_fn_out ()]
    runs a forked agent to completion and returns its final reply.

    Arguments:
    • [env] – the standard Eio environment obtained from [Eio_main.run].
      Only [env#net] and [env#cwd] are used internally.
    • [history] – full message history **before** the call to the
      [fork] tool.  The list is reused verbatim; no deep-copy is needed.
    • [call_id] – identifier of the tool invocation, echoed back in all
      generated [function_call_output] items so the parent can match
      streamed data to the correct call.
    • [arguments] – JSON payload passed to the tool, as received from the
      OpenAI API.  It is decoded with
      [Definitions.Fork.input_of_string] to extract the command and
      positional parameters given to the fork.
    • [tools] – flat list of available tools, forwarded unchanged to the
      nested completion request so the forked agent has access to the
      exact same capabilities as its parent.
    • [tool_tbl] – runtime dispatch table mapping tool names to
      implementations.  The table **must** contain an entry called
      ["fork"] that points back to {!execute}, otherwise recursive forks
      will fail with a key-error.
    • [on_event] – callback invoked for **every** low-level streaming
      event produced by the nested request, including tool-call
      arguments, deltas and reasoning sections.
    • [on_fn_out] – callback invoked whenever the fork produces a new
      [function_call_output] block.  This is necessary because assistants
      frequently stream partial answers before the final message ends.

    Optional parameters:
    • [?temperature] – sampling temperature forwarded to the nested
      request; defaults to the OpenAI model default.
    • [?max_output_tokens] – explicit upper bound on generated tokens.
    • [?reasoning] – whether reasoning items should be generated (see
      {!Openai.Responses.Request.Reasoning}).

    Returns: the concatenation of all assistant messages generated by the
    fork *after* the initial history, separated by newlines.

    The call is synchronous: the function only returns once the forked
    agent has either terminated normally or the OpenAI request hit its
    server-side token limit.

    Example – spawning a grep helper:
    {[
      let reply =
        Fork.execute
          ~env
          ~history:items
          ~call_id:"42"
          ~arguments:{|
            { "command": "grep", "arguments": ["-n", "let", "*.ml"] }
          |}
          ~tools
          ~tool_tbl
          ~on_event:(fun _ -> ())
          ~on_fn_out:(fun _ -> ())
          ()
      in
      print_endline reply
    ]} *)
val execute
  :  env:Eio_unix.Stdenv.base
  -> history:Openai.Responses.Item.t list
  -> call_id:string
  -> arguments:string
  -> tools:Openai.Responses.Request.Tool.t list
  -> tool_tbl:(string, string -> string) Base.Hashtbl.t
  -> on_event:(Openai.Responses.Response_stream.t -> unit)
  -> on_fn_out:(Openai.Responses.Function_call_output.t -> unit)
  -> ?temperature:float
  -> ?max_output_tokens:int
  -> ?reasoning:Openai.Responses.Request.Reasoning.t
  -> unit
  -> string

(** [history ~history ~arguments call_id] injects the synthetic
    *instruction* [function_call_output] item used by {!execute} at the
    end of [history] and returns the resulting list.

    This helper is primarily intended for unit-tests that need to verify
    the exact message sequence fed to the OpenAI client without running a
    full completion. *)
val history
  :  history:Openai.Responses.Item.t list
  -> arguments:string
  -> string
  -> Openai.Responses.Item.t list
