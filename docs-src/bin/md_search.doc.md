# md_search – semantic search over Markdown snippet indexes

`md_search` is a small command-line program that lets you run
natural-language queries over one or more *Markdown indexes* produced
with the companion [`md_index`](./md_index.doc.md) tool.

Each index folder contains three files generated at build time:

* `vectors.binio` – dense OpenAI embeddings of every snippet
* `snippets/ID.md` – the original text of the snippet (`ID` is the SHA-1
  of the text)
* `meta.json` – auxiliary metadata (not used by the searcher)

`md_search` loads the vectors in bulk, converts them into an
in-memory corpus with [`Vector_db`](../../lib/vector_db.md) and returns
the *k* closest snippets to your query.

---

## 1 Synopsis

```console
$ md-search --query TEXT [--index NAME|all] [--index-dir DIR] [-k INT]
```

The binary is installed under the same opam package as the library, so
`opam install ochat` will place it in your `$PATH`.

## 2 Command-line flags

| Flag | Default | Description |
|------|---------|-------------|
| `--query TEXT` | *(required)* | Natural-language query. Passed to the OpenAI embeddings endpoint. |
| `--index NAME` | `all` | Name of the index directory to search.  Use the special value `all` to automatically pick the five closest indexes based on their centroid vector. |
| `--index-dir DIR` | `.md_index` | Directory that stores the indexes as sub-folders. |
| `-k INT` | `5` | Number of snippets to print. |

## 3 Algorithm (high-level)

1. The query text is embedded with the model `text-embedding-ada-002`
   via `Openai.Embeddings.post_openai_embeddings`.
2. If `--index=all`, the program loads the global catalogue
   (`md_index_catalog.binio`) to locate the *five* indexes whose
   centroid vector is closest to the query (simple dot product).  If a
   specific name is supplied, the catalogue is skipped.
3. All vectors from the selected indexes are read from disk and fed to
   `Vector_db.create_corpus`, which normalises them and builds an Owl
   matrix.
4. Cosine similarity (computed with a matrix–vector product) ranks the
   snippets; the top-*k* IDs are mapped back to their source files.
5. Each snippet file is printed to `stdout` with a separators (`---`).

The entire pipeline runs inside an [Eio] fibre; blocking I/O does not
freeze the process.

## 4 Examples

Searching for a documentation snippet that explains *tail-call
optimisation*:

```console
$ md-search --query "tail call optimisation" -k 3
[1] [ocaml_manual] c172be…
… tail-recursive function …

---

[2] [blog_posts] 6a7338…
… trampoline avoids growing the stack …

---

[3] [stackoverflow] 4c0a12…
… perform TCO by re-writing the call as a loop …
```

Restricting the search to a single index:

```console
$ md-search --query "finalisers" --index ocaml_manual -k 2
```

## 5 Exit codes

| Code | Meaning |
|------|---------|
| 0 | At least one snippet printed. |
| 1 | Invalid arguments (missing `--query`). |
| 2 | No candidate index found. |
| 3 | OpenAI request failed. |
| 4 | No vectors present in the selected indexes. |

*(Codes > 1 are grouped under “non-zero” by the current implementation
but are listed here for future reference.)*

## 6 Limitations & notes

* **Internet access** – requires an environment variable `OPENAI_API_KEY`.
* **Memory usage** – the vectors of every selected index are loaded at once. Very large indexes (>100k snippets) may exhaust memory.
* **Embeddings model** – hard-coded to OpenAI *Ada 002*; vectors generated by other models are not currently supported.
* **Unix-only** – depends on `Eio.Posix`; does not compile to JavaScript or MirageOS.  
* **Preview length** – snippet preview is truncated to 8000 characters.

